# 六、让它具有互动性——创造用户体验

在前几章的课程中，我们已经学习了使用两种最常见的增强现实方法创建增强的要点:基于传感器的增强现实和基于计算机视觉的增强现实。我们现在能够在物理世界的视图上覆盖数字内容，支持增强现实跟踪，并处理帐户注册(在目标或室外)。

然而，我们只能在他们周围的增强世界中导航。允许用户也以直观的方式与虚拟内容交互不是很酷吗？用户交互是任何应用程序开发的主要组成部分。由于我们在这里关注的是用户与 3D 内容的交互(3D 交互)，因此可以开发以下三大类交互技术:

*   **导航**:围绕一个场景移动，选择一个特定的视点。在 AR 中，这种导航是通过物理运动来完成的(例如，在街上行走或转动桌子)，并且可以用额外的虚拟功能来补充(例如，地图视图、导航路径、冻结模式等)。
*   **操作**:选择、移动和修改对象。在增强现实中，这可以通过一系列传统方法(例如光线拾取)和新颖的交互范例(例如有形的用户界面)在物理和虚拟元素上完成。
*   **系统控制**:调整应用程序的参数，包括渲染、轮询过程和应用程序相关内容。在增强现实中，它可以对应于调整跟踪参数或可视化技术(例如，在增强现实浏览器中显示到您的兴趣点的距离)。

在本章中，我们将向您展示一些常用的增强现实交互技术的子集。我们将向您展示如何开发三种交互技术，包括光线拾取、基于邻近度的交互和基于 3D 运动手势的交互。这是设计增强现实应用程序的下一步，也是我们增强现实层的基础(参见[第 1 章](1.html "Chapter 1. Augmented Reality Concepts and Tools")、*增强现实概念和工具*)。

# 拾取棒-使用光线拾取进行三维选择

桌面上的 3D 交互电脑使用了有限的一组设备，包括键盘、鼠标或操纵杆(用于游戏)。在智能手机(或平板电脑)上，交互主要由触摸或传感器输入驱动。从交互输入(传感器数据，如屏幕上的 x 和 y 坐标，或事件类型，如点击或停留)，您可以开发不同的交互技术，如光线拾取、转向导航等。对于移动增强现实，一大套交互技术可以用于 2D 或 3D 交互。在本节中，我们将了解使用触摸输入结合名为 **光线拾取**的技术。

光线拾取的概念是使用从您的设备到您的环境(目标)的虚拟光线，并检测它沿途击中了什么。当你击中某个物体时(例如，光线与你的一个虚拟角色相交)，你可以考虑选择这个物体并开始操纵它。在这里，我们只看如何在 JME 挑选一个物体。在示例代码中，您可以扩展对象以支持进一步的操作，例如，当对象被击中和拾取时，您可以检测滑动触摸运动并平移对象，使其爆炸，或者将击中的对象用作某些游戏的拍摄光线，等等。

让我们开始吧。在 JME，你可以使用安卓专用的输入功能(通过`AndroidInput`)或者桌面应用程序中使用的相同功能(`MouseInput`)。默认情况下，安卓系统上的 JME 将任何触摸事件都映射为鼠标事件，这允许我们在安卓系统和你的桌面上使用(几乎)相同的代码。我们将为此项目选择以下解决方案；作为练习，你可以尝试使用`AndroidInput`(查看`AndroidTouchInputListener`使用`AndroidInput`)。

打开`RayPickingJME`示例。它使用与`VuforiaJME`相同的基本代码，我们的拣选方法基于一个 JME 的例子，关于这种拣选方法，请访问以下链接:[http://jmonkey engine . org/wiki/doku . PHP/jme 3:初学者:hello_picking](http://jmonkeyengine.org/wiki/doku.php/jme3:beginner:hello_picking) 。

首先要做的是在我们的`RayPickingJME`类中添加光线拾取所需的不同包:

```java
import com.jme3.math.Ray;
import com.jme3.collision.CollisionResult;
import com.jme3.collision.CollisionResults;
import com.jme3.input.MouseInput;
import com.jme3.input.controls.ActionListener;
import com.jme3.input.controls.KeyTrigger;
import com.jme3.input.controls.MouseButtonTrigger;
```

为了能够选择一个对象，我们需要在我们的`RayPicking`类的范围内声明一些全局变量:

*   `Node shootables`
*   `Geometry geom`

下一步是给我们班增加一个听众。如果你从未做过安卓或 JME 编程，你可能不知道什么是监听器。一个**监听器** 是一个事件处理技术，可以监听类中发生的任何活动，并提供特定的方法来处理任何事件。例如，如果您有一个鼠标按钮点击事件，您可以为它创建一个侦听器，它有一个`onPushEvent()`方法，您可以在其中安装自己的代码。在 JME，事件管理和侦听器被组织成两个组件，通过使用`InputManager`类进行控制:

*   **触发映射**:使用这个你可以将设备输入 can 和一个触发名称关联起来，比如点击鼠标可以关联`Press`或者`Shoot`或者`MoveEntity`等等。
*   **监听器**:使用这个可以将触发器名称与特定的监听器相关联；`ActionListener`(用于离散事件，如“按钮被按下”)或`AnalogListener`(用于连续事件，如操纵杆移动的幅度)。

因此，在您的`simpleInitApp`程序中，添加以下代码:

```java
  inputManager.addMapping("Shoot",      // Declare...
    newKeyTrigger(KeyInput.KEY_SPACE), // trigger 1: spacebar, or
    newMouseButtonTrigger(0));         // trigger 2: left-button click
  inputManager.addListener(actionListener, "Shoot");
```

因此，在这里，我们将按下空格键(甚至在使用虚拟键盘时)和鼠标点击(这是我们手机上的一个触摸动作)的情况映射到触发器名称`Shoot`。这个触发器名称与我们命名为`actionListener`的`ActionListener`事件监听器相关联。动作监听器将是我们进行光线拾取的地方；因此，在触摸屏设备上，通过触摸屏幕，您可以激活`actionListener`(使用触发器`Shoot`)。

我们的下一步是定义可能被我们的光线拾取击中的对象列表。一个很好的方法是将它们重组到一个特定的组节点下。在下面的代码中，我们将创建一个 box 对象，并将其放置在名为`shootables`的组节点下:

```java
Box b = new Box(7, 4, 6); // create cube shape at the origin
geom = new Geometry("Box", b);  // create cube geometry from the shape
Material mat = new Material(assetManager,
"Common/MatDefs/Misc/Unshaded.j3md");  // create a simple material
mat.setColor("Color", ColorRGBA.Red);   // set color of material to blue
geom.setMaterial(mat);        // set the cube's material
geom.setLocalTranslation(new Vector3f(0.0f,0.0f,6.0f));

shootables = new Node("Shootables");
shootables.attachChild(geom);
rootNode.attachChild(shootables);
```

现在我们有了我们的触摸映射和我们可以被击中的物体。我们只需要实现我们的听众。JME 的光线投射方式与其他许多图书馆相似；我们使用命中坐标(在屏幕坐标中定义)，使用我们的相机转换它们，创建一条光线，并运行命中。在我们的增强现实例子中，我们将使用增强现实摄像机，它是由我们基于计算机视觉的跟踪器`fgCam`更新的。所以，AR 中的代码与另一个虚拟游戏中的代码相同，只是在这里，我们的相机位置由跟踪器更新。

我们创建一个`Ray`对象，并通过为我们的列表对象调用`collideWith`来运行一个挑选测试(命中测试)，该列表对象可以被命中(`shootables`)。碰撞结果将存储在`CollisionResults`对象中。因此，我们的侦听器代码看起来像下面的代码:

```java
  privateActionListeneractionListener = new ActionListener() {

  public void onAction(String name, booleankeyPressed, float tpf) {
      Log.d(TAG,"Shooting.");

      if (name.equals("Shoot") && !keyPressed) {

        // 1\. Reset results list.
        CollisionResults results = new CollisionResults();

        // 2\. Mode 1: user touch location.
        Vector2f click2d = inputManager.getCursorPosition();
        Vector3f click3d = fgCam.getWorldCoordinates(
        new Vector2f(click2d.x, click2d.y), 0f).clone();
        Vector3f dir = fgCam.getWorldCoordinates(
        new Vector2f(click2d.x, click2d.y), 1f).subtractLocal(click3d).normalizeLocal();
        Ray ray = new Ray(click3d, dir);

        // 2\. Mode 2: using screen center
        //Aim the ray from fgcamloc to fgcam direction.
        //Ray ray = new Ray(fgCam.getLocation(), fgCam.getDirection());

        // 3\. Collect intersections between Ray and Shootables in results list.
        shootables.collideWith(ray, results);
…
```

那么，我们如何处理结果呢？正如本书前面所解释的，你可以用不同的方式来操纵它。我们将在这里做一些简单的事情；我们将检测我们的盒子是否被选中，如果被选中，则将的颜色更改为红色表示没有交叉点，如果有交叉点，则更改为绿色。我们先打印结果进行调试，在这里可以使用`getCollision()`功能检测到哪个物体被击中(`getGeometry()`)、距离有多远(`getDistance()`)以及接触点(`getContactPoint()`):

```java
  for (int i = 0; i<results.size(); i++) {
    // For each hit, we know distance, impact point, name of geometry.
    floatdist = results.getCollision(i).getDistance();
    Vector3f pt = results.getCollision(i).getContactPoint();
    String hit = results.getCollision(i).getGeometry().getName();

    Log.d(TAG,"* Collision #" + i + hit);
    //         Log.d(TAG,"  You shot " + hit + " at " + pt + ", " + dist + "wu away.");
  }
```

因此，通过使用前面的代码，我们可以检测我们是否有任何结果，并且由于我们的场景中只有一个对象，我们认为如果我们命中，这是我们的对象，因此我们将对象的颜色更改为绿色。如果我们没有被击中，因为只有我们的对象，我们把它变成红色:

```java
  if (results.size() > 0) {
    // The closest collision point is what was truly hit:
  CollisionResult closest = results.getClosestCollision();

  closest.getGeometry().getMaterial().setColor("Color", ColorRGBA.Green);
  } else {
    geom.getMaterial().setColor("Color", ColorRGBA.Red);
  }
```

您应该会得到类似于下面截图所示的结果(点击:左，错过:右):

![Pick the stick – 3D selection using ray picking](img/8553OS_06_01a.jpg)

现在，您可以部署并运行该示例；触摸屏幕上的物体，看到我们的盒子在变色！

# 基于邻近的交互

AR 中另一种类型的交互是使用相机和物理对象之间的关系。如果你有一个放在桌子上的目标，你用你的设备从不同的角度观察一个虚拟物体，你也可以用它来创造互动。这个想法很简单:你可以检测到你的相机(在你的移动设备上)和你的目标(放在桌子上)之间空间变换的任何变化，并触发一些事件。例如，您可以检测相机是否在特定角度下，它是否从上方看着目标，等等。

在这个例子中，我们将实现一个**邻近**技术，可以用来创建一些很酷的动画和效果。邻近技术使用增强现实摄像机和基于计算机视觉的目标之间的距离。

所以，在你的 Eclipse 中打开`ProximityBasedJME`项目。同样，这个项目也是基于`VuforiaJME`的例子。

首先，我们使用三种不同的颜色(红色、绿色和蓝色)创建三个对象(一个盒子、一个球体和一个圆环体)，如下所示:

```java
    Box b = new Box(7, 4, 6); // create cube shape at the origin
    geom1 = new Geometry("Box", b);  // create cube geometry from the shape
    Material mat = new Material(assetManager,"Common/MatDefs/Misc/Unshaded.j3md");  // create a simple material
    mat.setColor("Color", ColorRGBA.Red);   // set color of material to red
    geom1.setMaterial(mat);                   // set the cube's material

    geom1.setLocalTranslation(new Vector3f(0.0f,0.0f,6.0f));

    rootNode.attachChild(geom1);              // make the cube appear in the scene

    Sphere s = new Sphere(12,12,6);
    geom2 = new Geometry("Sphere", s);  // create sphere geometry from the shape
    Material mat2 = new Material(assetManager,"Common/MatDefs/Misc/Unshaded.j3md");  // create a simple material
    mat2.setColor("Color", ColorRGBA.Green);   // set color of material to green
    geom2.setMaterial(mat2);                   // set the sphere's material

    geom2.setLocalTranslation(new Vector3f(0.0f,0.0f,6.0f));

    rootNode.attachChild(geom2);              // make the sphere appear in the scene

    Torus= new Torus(12, 12, 2, 6); // create torus shape at the origin
    geom3 = new Geometry("Torus", t);  // create torus geometry from the shape
    Material mat3 = new Material(assetManager,"Common/MatDefs/Misc/Unshaded.j3md");  // create a simple material
    mat3.setColor("Color", ColorRGBA.Blue);   // set color of material to blue
    geom3.setMaterial(mat3);                   // set the torus material
    geom3.setLocalTranslation(new Vector3f(0.0f,0.0f,6.0f));

    rootNode.attachChild(geom3);              // make the torus appear in the scene
```

在大量的场景图库中，你经常会发现一个切换节点，它允许基于一些参数来切换对象的表示，比如对象到摄像机的距离。JME 没有开关节点，所以我们将模拟它的行为。我们将根据对象与摄像机的距离来更改显示哪个对象(长方体、球体或圆环体)。简单的方法是添加或删除不应该在一定距离显示的对象。

为了实现邻近技术，我们查询我们的增强现实摄像机的位置(`fgCam.getLocation()`)。从那个位置，你可以计算到一些物体或目标的距离。根据定义，到目标的距离类似于摄像机位置的距离(用三维向量表示)。因此，我们要做的是为我们的对象定义如下三个范围:

*   **相机距离 50 及以上**:显示立方体
*   **相机距离 40-50** :显示球体
*   **摄像机距离低于 40** :显示圆环

`simpleUpdate`方法中的结果代码相当简单:

```java
      Vector3f pos=new Vector3f();

      pos=fgCam.getLocation();

      if (pos.length()>50.)
      {
        rootNode.attachChild(geom1);           
        rootNode.detachChild(geom2);       
        rootNode.detachChild(geom3); 

      }
      else
        if (pos.length()>40.)
        {
          rootNode.detachChild(geom1);           
          rootNode.attachChild(geom2);       
          rootNode.detachChild(geom3); 
        },
        else
        {
          rootNode.detachChild(geom1);           
          rootNode.detachChild(geom2);       
          rootNode.attachChild(geom3); 
        }
```

运行您的示例，并更改设备到跟踪目标的距离。这将影响所呈现的对象。当你远离时会出现一个立方体(如下图左侧所示)，当你靠近时会出现一个圆环体(如下图右侧所示)，两者之间会出现一个球体(如下图中央所示):

![Proximity-based interaction](img/553OS_06_02a.jpg)

# 使用加速度计的简单手势识别

在[第 4 章](4.html "Chapter 4. Locating in the World") *定位于世界*中，你被介绍到典型安卓设备内置的各种传感器。您学习了如何使用它们来导出设备的方向。然而，使用这些传感器，尤其是加速度计，你可以做得更多。如果你曾经玩过 Wii 游戏，你肯定会被挥舞 Wiimote(例如，打网球或高尔夫 Wii 游戏时)可以实现的自然交互迷住。有趣的是，Wiimate 使用了与许多安卓智能手机相似的加速度计，所以你实际上可以实现与 Wiimate 相似的交互方法。对于复杂的 3D 运动手势(例如在空中画一个 8 字形)，您将需要一些机器学习背景或访问使用库，例如以下链接中的库:[http://www . dfki . de/~ rnesel/tools/手势 _ recognition/手势 _recognition.html](http://www.dfki.de/~rnessel/tools/gesture_recognition/gesture_recognition.html) 。然而，如果你只想识别简单的手势，你可以用几行代码轻松做到。接下来，我们将向您展示如何识别简单的手势，如摇动手势，即快速来回挥动手机几次。

如果你看一下样本项目`ShakeItJME`，你会注意到它在很大程度上与[第四章](4.html "Chapter 4. Locating in the World")、*定位于世界*中的`SensorFusionJME`项目相同。事实上，我们只需要执行几个简单的步骤就可以扩展任何已经使用加速度计的应用。在`ShakeItJMEActivity`中，您首先添加一些与抖动检测相关的变量，主要包括用于存储加速度计事件时间戳的变量(`mTimeOfLastShake`、`mTimeOfLastForce`和`mLastTime`)、用于存储过去加速度计力的变量(`mLastAccelValX`、`mLastAccelValY`和`mLastAccelValZ`)以及多个抖动持续时间阈值、超时值(`SHAKE_DURATION_THRESHOLD`、`TIME_BETWEEN_ACCEL_EVENTS_THRESHOLD`和`SHAKE_TIMEOUT`)和最小数量的加速度计力和传感器样本(`ACCEL_FORCE_THRESHOLD`和`ACCEL_EVENT_COUNT_THRESHOLD`)。

接下来，您只需在代码的`Sensor.TYPE_ACCELEROMETER`部分添加对您的`SensorEventListener::onSensorChanged`方法中的`detectShake()`方法的调用。

`detectShake()`方法是你的抖动检测的核心:

```java
public void detectShake(SensorEvent event) {
…
  floatcurAccForce = Math.abs(event.values[2] - mLastAccelValZ) / timeDiff;
  if (curAccForce> ACCEL_FORCE_THRESHOLD) {
    mShakeCount++ ;
    if ((mShakeCount>= ACCEL_EVENT_COUNT_THRESHOLD) && (now - mTimeOfLastShake> SHAKE_DURATION_THRESHOLD)) {
      mTimeOfLastShake = now;mShakeCount = 0;			
      if ((com.ar4android.ShakeItJME) app != null) {
        ((com.ar4android.ShakeItJME) app).onShake();
      }
    }
…
  }    
}
```

在这种方法中，基本上是检查某个时间段内的加速度计值是否大于阈值。如果是，你调用你的 JME 应用的`onShake()`方法并将事件集成到你的应用`logic.Note`中，在这个例子中，我们只使用 z 方向的加速度计值，也就是说，平行于摄像机指向的方向。通过将加速度计的 x 和 y 值合并到`curAccForce`的计算中，您可以很容易地将其扩展到包括侧向震动运动。作为如何使用抖动检测触发事件的示例，在您的 JME 应用程序的`onShake()`方法中，我们触发了一个新的行走忍者动画:

```java
public void onShake() {
  mAniChannel.setAnim("Spin");
}
```

为了避免忍者现在一直旋转；旋转动画结束后，我们将切换到行走动画:

```java
public void onAnimCycleDone(AnimControl control, AnimChannel channel, String animName) {
if(animName.contains("Spin")) {
    mAniChannel.setAnim("Walk");
  }
}
```

如果你现在启动你的应用程序，沿着查看方向摇动设备，你应该会看到忍者是如何停止行走并轻轻旋转的，就像下图所示的:

![Simple gesture recognition using accelerometers](img/8553OS_06_03.jpg)

# 总结

在本章中，我们向您介绍了三种交互技术，适用于各种各样的增强现实应用。拾取允许您通过触摸屏幕来选择 3D 对象，就像您在 2D 选择中所做的那样。基于邻近度的相机技术允许您试验设备的距离和方向，以触发应用程序事件。最后，我们向您展示了一个 3D 手势检测方法的简单示例，它可以为您的应用程序增加更多的交互可能性。这些技术应该作为您创建自己的交互方法的基础，以您的特定应用场景为目标。在最后一章，我们将介绍一些先进的技术和进一步的阅读，以帮助您充分利用您的增强现实应用程序。